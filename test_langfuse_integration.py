#!/usr/bin/env python3
"""
Test script to verify Langfuse integration and DeepEval optimization
Run this to ensure the new architecture is working correctly
"""

import sys
import os
from pathlib import Path

# Add parent directory to path
sys.path.append(str(Path(__file__).parent))

def test_langfuse_import():
    """Test that Langfuse can be imported"""
    print("Testing Langfuse import...")
    try:
        from evaluation.langfuse_config import (
            langfuse_observer, 
            LangfuseConfig, 
            ObservabilityLevel
        )
        print("‚úÖ Langfuse modules imported successfully")
        return True
    except ImportError as e:
        print(f"‚ùå Failed to import Langfuse: {e}")
        return False

def test_deepeval_import():
    """Test that DeepEval can be imported"""
    print("\nTesting DeepEval import...")
    try:
        from evaluation.deepeval_optimizer import (
            HallucinationOptimizer,
            OptimizationConfig,
            ChainOfThoughtTemplates
        )
        print("‚úÖ DeepEval optimizer imported successfully")
        return True
    except ImportError as e:
        print(f"‚ùå Failed to import DeepEval optimizer: {e}")
        return False

def test_langfuse_configuration():
    """Test Langfuse configuration from environment"""
    print("\nTesting Langfuse configuration...")
    try:
        from evaluation.langfuse_config import LangfuseConfig
        
        config = LangfuseConfig.from_env()
        print(f"  ‚Ä¢ Host: {config.host}")
        print(f"  ‚Ä¢ Enabled: {config.enabled}")
        print(f"  ‚Ä¢ Observability Level: {config.observability_level.value}")
        print(f"  ‚Ä¢ Sampling Rate: {config.sampling_rate}")
        
        if not config.public_key or not config.secret_key:
            print("‚ö†Ô∏è  Langfuse API keys not configured (add to .env for production)")
        else:
            print("‚úÖ Langfuse configuration loaded")
        return True
    except Exception as e:
        print(f"‚ùå Failed to configure Langfuse: {e}")
        return False

def test_basic_trace():
    """Test creating a basic trace with Langfuse"""
    print("\nTesting basic trace creation...")
    try:
        from evaluation.langfuse_config import langfuse_observer
        from langfuse.decorators import observe, langfuse_context
        
        @observe(name="test_function")
        def sample_function(input_text: str) -> str:
            # Simulate some processing
            result = f"Processed: {input_text}"
            
            # Add a score
            try:
                langfuse_context.score_current_trace(
                    name="test_score",
                    value=0.95,
                    comment="Test score"
                )
            except:
                pass  # Scoring might fail without API keys
            
            return result
        
        # Run the function
        result = sample_function("Hello, Langfuse!")
        print(f"  ‚Ä¢ Function result: {result}")
        print("‚úÖ Basic trace creation works")
        return True
    except Exception as e:
        print(f"‚ùå Failed to create trace: {e}")
        return False

def test_optimization_config():
    """Test DeepEval optimization configuration"""
    print("\nTesting optimization configuration...")
    try:
        from evaluation.deepeval_optimizer import OptimizationConfig
        
        config = OptimizationConfig(
            max_iterations=5,
            target_hallucination_score=0.95,
            enable_cot=True,
            cot_style="structured"
        )
        
        print(f"  ‚Ä¢ Max iterations: {config.max_iterations}")
        print(f"  ‚Ä¢ Target hallucination score: {config.target_hallucination_score}")
        print(f"  ‚Ä¢ Chain-of-Thought enabled: {config.enable_cot}")
        print(f"  ‚Ä¢ CoT style: {config.cot_style}")
        print("‚úÖ Optimization configuration works")
        return True
    except Exception as e:
        print(f"‚ùå Failed to create optimization config: {e}")
        return False

def test_cot_templates():
    """Test Chain-of-Thought templates"""
    print("\nTesting Chain-of-Thought templates...")
    try:
        from evaluation.deepeval_optimizer import ChainOfThoughtTemplates
        
        templates = ChainOfThoughtTemplates()
        base_prompt = "Answer the following question: {input}"
        
        # Test structured template
        structured = templates.STRUCTURED.format(base_prompt=base_prompt)
        assert "step-by-step" in structured.lower()
        print("  ‚Ä¢ Structured template: OK")
        
        # Test narrative template
        narrative = templates.NARRATIVE.format(base_prompt=base_prompt)
        assert "reasoning process" in narrative.lower()
        print("  ‚Ä¢ Narrative template: OK")
        
        # Test hybrid template
        hybrid = templates.HYBRID.format(base_prompt=base_prompt)
        assert "systematic approach" in hybrid.lower()
        print("  ‚Ä¢ Hybrid template: OK")
        
        print("‚úÖ Chain-of-Thought templates work")
        return True
    except Exception as e:
        print(f"‚ùå Failed to test CoT templates: {e}")
        return False

def test_mock_optimization():
    """Test a mock optimization without actual LLM calls"""
    print("\nTesting mock optimization...")
    try:
        from evaluation.deepeval_optimizer import HallucinationOptimizer, OptimizationConfig
        
        # Create optimizer with minimal config
        config = OptimizationConfig(
            max_iterations=2,  # Just 2 iterations for testing
            target_hallucination_score=0.90,
            enable_cot=True
        )
        
        optimizer = HallucinationOptimizer(config)
        
        # Create simple test case
        test_cases = [
            {
                "input": "What is 2+2?",
                "expected_output": "4",
                "context": ["Basic arithmetic: 2+2=4"]
            }
        ]
        
        # Test prompt variation generation
        variation = optimizer._generate_prompt_variation(
            current_prompt="Answer: {input}",
            iteration=0,
            history=[],
            few_shot_examples=None
        )
        
        print(f"  ‚Ä¢ Generated prompt variation")
        print(f"  ‚Ä¢ Config temperature: {variation['config']['temperature']}")
        print(f"  ‚Ä¢ Config top_p: {variation['config']['top_p']}")
        print("‚úÖ Mock optimization works")
        return True
    except Exception as e:
        print(f"‚ùå Failed mock optimization: {e}")
        return False

def main():
    """Run all tests"""
    print("""
    ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
    ‚ïë           PromptForge Langfuse Integration Test Suite           ‚ïë
    ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
    """)
    
    tests = [
        ("Langfuse Import", test_langfuse_import),
        ("DeepEval Import", test_deepeval_import),
        ("Langfuse Configuration", test_langfuse_configuration),
        ("Basic Trace", test_basic_trace),
        ("Optimization Config", test_optimization_config),
        ("CoT Templates", test_cot_templates),
        ("Mock Optimization", test_mock_optimization)
    ]
    
    results = []
    for name, test_func in tests:
        try:
            passed = test_func()
            results.append((name, passed))
        except Exception as e:
            print(f"‚ùå Test '{name}' crashed: {e}")
            results.append((name, False))
    
    # Summary
    print("\n" + "="*70)
    print("TEST SUMMARY")
    print("="*70)
    
    passed = sum(1 for _, p in results if p)
    total = len(results)
    
    for name, passed_test in results:
        status = "‚úÖ PASS" if passed_test else "‚ùå FAIL"
        print(f"{status:8} | {name}")
    
    print("-"*70)
    print(f"Results: {passed}/{total} tests passed ({passed/total*100:.0f}%)")
    
    if passed == total:
        print("\nüéâ All tests passed! The Langfuse integration is working correctly.")
    elif passed > 0:
        print(f"\n‚ö†Ô∏è  {total - passed} test(s) failed. Check the output above for details.")
        print("Note: Some failures are expected if API keys are not configured.")
    else:
        print("\n‚ùå All tests failed. Please check your installation.")
    
    return 0 if passed == total else 1

if __name__ == "__main__":
    sys.exit(main())